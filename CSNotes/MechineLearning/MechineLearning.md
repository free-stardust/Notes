## 吴恩达机器学习
---
### 1 绪论：初识机器学习
#### 1.1 应用领域
- **数据挖掘**
    - 网页点击数据；
    - 医疗记录；
    - 生物工程等数据集；
- **无法编写的程序**
    - 自动驾驶；
    - 手写体识别；
    - 自然语言处理；
    - 计算机视觉；
- **私人定制程序**
    - 智能推荐；
- **理解人类的学习过程和大脑**
#### 1.2 什么是机器学习
- **Arthur Samuel在1959年的定义**：在没有明确设置的情况下，是计算机具有学习能力的领域；
- **Tom Mitchell在1998年的定义**：计算机程序从经验E中学习，解决某一项任务T，进行某一性能度量P，通过P测定在T上的表现因经验而提高；
- #### 1.3 监督学习和无监督学习
- **监督学习 (Supervised Learning)**
    - 根据给定的算法和数据集，且该数据集包含了正确的答案，便可以对应给出任意给定一个数据的对应答案，即解决的是**回归问题**或者**分类问题**；
- **无监督学习 (Unsupervised Learning)**
    - 给算法大量的数据，要求其找出数据结构，即解决的是**聚类问题**；

### 2 单变量线性回归
#### 2.1 代价函数
- **假设函数**
    $$h_\theta(x)=\theta_0+\theta_1x$$
- **参数**
    $$\theta_0,\theta_1$$
- **代价函数**
    $$J(\theta_0,\theta_1)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^{2}$$
- **目标**
    $$\underset{\theta_0,\theta_1}{minimize}\ J(\theta_0,\theta_1)$$
    ><font size=2>**说明**：此处之所以是$\frac{1}{2m}$，原因是在按梯度下降法求最小值点时刚好可以把平方的2约掉，从而变为$\frac{1}{m}$，且不论是$\frac{1}{m}$，还是$\frac{1}{2m}$，最后计算得出的$\theta_0$、$\theta_1$都是一样的。</font>
#### 2.2 梯度下降法
- **算法描述**
    $$\begin{array}{lc} 
        repeat\ until\ convergence\ \{\\
        \ \ \ \ \theta_j:=\theta_j-\alpha\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)\ \ \ \ (for j=0\ and\ j = 1 \\
        \}
    \end{array}$$
    > <font size=2>**说明**：此处的“:=”表示赋值运算。</font>
- **同步更新**
    $$\begin{array}{l} 
        temp0:=\theta_0-\alpha\frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1) \\ 
        temp1:=\theta_1-\alpha\frac{\partial}{\partial \theta_1}J(\theta_0, \theta_1) \\ 
        \theta_0:=temp0 \\ 
        \theta_1:=temp1
    \end{array}$$
- **$J(\theta_0, \theta_1)$的偏导数**
    $$\frac{\partial}{\partial \theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})$$

    $$\frac{\partial}{\partial \theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)}$$
### 3 线性代数回顾
#### 3.1 需要用到的线性代数基础知识
><font size=2>**说明**：本部分回顾的线代都很基础，没有涉及深入的线性代数知识，且学习本课程理解基本原理需要这些知识足以，但对于个别公式推导需要的线性代数知识则超过本部分复习的基础知识，需要课下自行学习，比如矩阵的求导的知识。</font>
- 矩阵和向量的概念；
- 矩阵的加法和乘法运算；
- 矩阵乘法的性质;
- 逆矩阵和转置；
### 4 环境配置
#### 4.1 Octave安装及环境配置
- **下载地址**
    ><font size=2>**注意**：达叔说不要安装Octave4.0版本。</font>
  - [Microsoft Windows下载地址](http://wiki.octave.org/Octave_for_Microsoft_Windows)
  - [macOS下载地址](http://wiki.octave.org/Octave_for_macOS)
  - [其他环境适配版本下载地址](http://wiki.octave.org/Category:Installation)
- **注意事项**
    - **重要**：第一次使用前先运行post-install.bat文件；
    - 启动方式是octave.vbs，可以新建快捷键到其他地方，在快捷方式文件属性的目标文件后添加“--no-gui”打开后便是命令窗口视图；
#### 4.2 Matlab安装及环境配置
- **下载地址**
  - [Matlab2020b百度网盘下载地址](https://pan.baidu.com/share/init?surl=XegUi8JgN1fgG_cPqWSTPA)
  - 提取码：j826
  - 安装教程见[脚本之家](https://www.jb51.net/softs/745941.html) 
### 5 多变量线性回归
#### 5.1 多元梯度下降法
- **假设函数**
    $$h_\theta(x)=\theta^{T}x=\theta_0x_0+\theta_1x_1+\theta_2x_2+\cdot\cdot\cdot+\theta_nx_n$$
- **参数**
    $$\theta_0,\theta_1,\ldots,\theta_n$$
- **代价函数**
    $$J(\theta_0,\theta_1,\ldots,\theta_n)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^{2}$$
    ><font size=2>**注意**：上式中的$x^{(i)}$和$y^{(i)}$表示的是第$i$个向量，每个向量含有$n$个维度。</font>
- **梯度下降算法(n≥1)**
    $$\begin{array}{l}
        Repeat\ \{\\
        \ \ \ \ \theta_j:=\theta_j-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}\ \ \ (for\ j=0,1,2,\ldots,n)\\
        \}\\
        \ \\
        \theta_0:=\theta_0-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{0}^{(i)}\\
        \theta_1:=\theta_1-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{1}^{(i)}\\
        \theta_2:=\theta_2-\alpha\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{2}^{(i)}\\
        \ldots
    \end{array}$$
    ><font size=2>**注意A**：$x_{j}^{(i)}$表示的是第$i$个向量中的第$j$维的值；</br> **注意B**：$\frac{\partial}{\partial \theta_j}J(\theta_0,\theta_1,\ldots,\theta_n)=\frac{1}{m}\sum\limits_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_{j}^{(i)}$。</font>
#### 5.2 梯度下降法实用技巧A——特征缩放
- **目的**
  - 使不同特征得取值处在相近的范围，使梯度下降法更快的收敛；
- **做法**
    ><font size=2>**注意A**：一般将特征值的取值约束到-1到+1的范围内;</br>**注意B**：实际上是约束到和该范围同数量级的其他范围也是可以的，比如0到3或者-2到+0.5都是可以的。</font>
    - 用特征值除以最大值；
    - 归一化处理，比如一般分布的正态归一化；
#### 5.3 梯度下降法B——学习率的选择
- **什么是学习率？**
  - 学习率为梯度下降算法中的$\alpha$；
  - 在梯度下降算法中，如果**学习率选择的太大**，则**代价函数$J(\theta)$可能不会每次都下降**；如果**学习率选择的太小**，则**代价函数$J(\theta)$可能收敛缓慢**，效率低下；
- **收敛的判断**
  - 通过观察代价函数对应不同的迭代次数的函数图像进行判断；
  - 通过自动检测函数，设定合适的阈值$\varepsilon$，代价函数的变化值小于该阈值则收敛，但该方法很难确定合适的阈值$\varepsilon$;
- **学习率的选择**
  - 一般按以下值从小到大一次尝试，尝试过程中按10倍上个学习率进行迭代；
  $$\ldots,0.001,0.01,0.1,1,\ldots$$
  - **达叔的选择**是按3倍上个迭代的学习率进行迭代；
  $$\ldots,0.001,0.003,0.01,0.03,0.1,0.3,1,\ldots$$
#### 5.4 正规方程法(区别于迭代方法的直接解法)
- **前置条件**
  - **$m$个样本**
  $$(x^{(1),y^{(1)}}),(x^{(2)},y^{(2)}),(x^{(3),y^{(3)}}),\ldots,(x^{(m)},y^{(m)})$$
  - **$n$个特征**
  ><font size=2>**说明**：每个$x^{(i)}$向量均含有$n+1$个维度，其中$x_{0}^{(i)}$为常数$1$，其余$n$个维度则表示$n$个特征的样本值。</font>

  $$x^{(i)}=
  \left[ \begin{array}{c}
      x_{0}^{(i)}\\
      x_{1}^{(i)}\\
      x_{2}^{(i)}\\
      \vdots\\
      x_{n}^{(i)}
  \end{array} \right]
  \in R^{n+1}$$
- **构建矩阵$X$和向量$y$**
    ><font size=2>**注意**：矩阵$X$为$m\times(n+1)$的矩阵，$y$为$m$维向量。</font>

  $$X=
  \left[ \begin{array}{c}
      (x^{1})^{T}\\
      (x^{2})^{T}\\
      (x^{3})^{T}\\
      \vdots\\
      (x^{m})^{T}
  \end{array} \right]
  \ \ and\ \  
  y=
  \left[ \begin{array}{c}
      y^{(1)}\\
      y^{(2)}\\
      y^{(3)}\\
      \vdots\\
      y^{(m)}
  \end{array} \right]$$
- **利用正规方程法计算$\theta$向量**
  - **参数$\theta$向量**
  $$\theta=
  \left[ \begin{array}{c}
      \theta_0\\
      \theta_1\\
      \theta_2\\
      \vdots\\
      \theta_n
  \end{array} \right]$$
  - **正规方程计算参数$\theta$向量**
  $$\theta=(X^{T}X)^{-1}X^{T}y$$
  - **Octave实现正规方程的计算语句**
    ```Matlab
    pinv (X'*X)*X'*y
    ```
  - **正规方程直接计算参数$\theta$向量**
    - 待定……
- **梯度下降法和正规方程法的优缺点**
  - **梯度下降法**
    - **优点**
      - 在特征变量很多的情况下，也能运行的很好；
    - **缺点**
      - 需要选择并尝试不同的学习率$\alpha$，找到运行的最好的哪个;
      - 需要更多次迭代，且取决于具体细节，计算可能会更慢；
  - **正规方程法**
    - **优点**
      - 不需要选择学习效率$\alpha$;
      - 不需要迭代；
    - **缺点**
      - 求解参数$\theta$向量需要计算$(X^{T}X)^{-1}$，且对大多数计算应用来说，实现逆矩阵计算的代价，以矩阵维度的三次方增长，即时间复杂度为$O(n^{3})$，故如果特征变量很多时，计算会慢很多；
  - **计算方法的选择**
    - 以$10000$为分界线，当特征变量的维数$n$为$10000$左右的时候，正规方程计算参数$\theta$的优势不在明显；
    - 当特征变量的维数为$100$和$1000$这样低于$10000$的数量时，正规方程计算参数$\theta$更具优势；
    - 当特征变量的维数远大于$10000$，比如为$10^{6}$方时，梯度下降法更具优势；
#### 5.5 正规方程在矩阵不可逆情况下的解决方法
- **伪逆矩阵**
  - 逆矩阵的广义形式；
  - 由于奇异矩阵(不可逆的方阵矩阵)和非方阵的矩阵不存在逆矩阵，但是在Matlab或者Octave中可以使用pinv(A)求这两种矩阵的伪逆矩阵(且inv(A)伪求逆矩阵的函数)；
- **$X^{T}X$不可逆的可能原因及处理方法**
  - **原因A**
    - 由于某些原因，学习问题包含了多余的特征，比如某些特征线性相关；
      - **示例**：在预测房价的示例中，如果特征变量$x_1$使用平方英尺为单位，而特征变量$x_2$使用平方米为单位，这种情况下得出的$X^{T}X$通常是不可逆的；
    - **处理方法**：删除多组(如果有很多组的话)具有线性关系的两个特征中的一个，直到没有多余的特征为止；
  - **原因B**
    - 运行的学习算法有很多特征，具体来说例如样本数小于等于特征变量数，即$m\leq n$；
      - **示例**：比如让$m=10$，$n=100$时，要找到合适的参数向量$\theta$(该向量为$n+1$维)，即使用10个样本找到101个参数值，有时可能会成功，但不具有普遍性；
    - **处理方法**：删除一些特征；或者使用正则化的方法(这个方法后面会学到)；
### 6 Octave/Matlab 教程
#### 6.1 基本操作
>**说明**：Octave的语法和Matlab基本相同，此处以使用Octave操作为例。
```Matlab
octave:1>                                 % 默认的Octave命令窗口显示内容
octave:2> PS1 >>                          % PS1指令切换Octave窗口提示符的显示内容为“>>”
>> a = 1
a = 1
>> a = 3;                                 % 语句末尾加了分号“;”，命令窗口就不会答应输出结果
>> a = pi
a = 3.1416                                % 默认输出格式
>> disp(a)
3.1416
>> disp(sprintf('2 decimals: %0.2f', a))  % 传统C语言的格式化输出 
2 decimals: 3.14
>> format long                            % 改变小数显示默认的位数
>>a
a = 3.14159265358979
>> format short
a = 3.1416
>> A = [1 2; 3 4; 5 6]                    % 构建一个3×2矩阵
A =

   1   2
   3   4
   5   6

>> A = [1 2;                              % 构建一个矩阵的另一种写法
>3 4;
>5 6]
A =

   1   2
   3   4
   5   6

>> V = [1 2 3]                            % 构建一个1×3的行向量
V =

   1   2   3

>> V = [1; 2; 3]                          % 构建一个3×1的列向量
V =

   1
   2
   3

>> V = 1:0.2:2                            % 按照0.2的步长构建一个范围为1到2的1×6的行向量
V =

    1.0000    1.2000    1.4000    1.6000    1.8000    2.0000

>> V = 1:6                                % 构建一个1到6逐步递增的行向量
V =

    1   2    3   4   5   6

>> ones(2,3)                              % 构建一个各项都为1的2×3矩阵
ans =

    1   1   1
    1   1   1

>> C = 2 * ones(2,3)                      % 构建一个各项都为2的2×3矩阵
C =

    2   2   2
    2   2   2

>> W = rand(3,3)                          % 构建一个各项值介于0到1之间的3×3随机数矩阵
W =

   0.112010   0.388236   0.744595
   0.046368   0.174028   0.427191
   0.321745   0.147818   0.804832 

>> W = randn(1,3)                         % 构建一个各项值符合高斯随机分布(正态分布)1×3矩阵
W =

  -0.9340  -0.8005  -0.1647

>> W = -6 + sqrt(10)*(randn(1,10000));    % 一个复杂的示例
>> hist(W)                                % 绘制直方图
>> hist(W, 50)                            % 绘制具有50竖条的直方图
>> eye(4)                                 % 构建一个4阶单位矩阵
ans =

Diagonal Matrix

   1   0   0   0
   0   1   0   0
   0   0   1   0
   0   0   0   1

>> help eye                               % 查看eye的帮助信息
>> help help                              % 查看帮助的使用方法
>> help rand                              % 产看rand的帮助信息
```
![hist(W)绘制的直方图(左) hist(W,50)绘制的直方图(右)](image/hist_w.png)
#### 6.2 移动数据